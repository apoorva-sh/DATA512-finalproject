{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Relationship between Music and Emotion\n",
    "## Project Plan\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "Apoorva Shetty\n",
    "\n",
    "DATA 512 - Human Centered Data Science\n",
    "\n",
    "University of Washington, Fall 2019\n",
    "\n",
    "\n",
    "# Introduction\n",
    "-----------------------------\n",
    "\n",
    "Exploring the way our brain makes connections that illicit certain responses has been a common source of intrigue within research groups. How often do you find yourself hunting down that old song you've forgotten the name of just to feel like you were back in a certain period of your life? Or do you have a specific playlist that helps you \"focus\" more? The connection between emotions felt by someone and music opens doors to how external stimuli adds to a memory or an emotion and how our brain accesses it. \n",
    "\n",
    "The dataset I'm hoping to explore looks at how humans respond to a music clip that is mildly altered to illicit certain emotional responses, I think such research can help us introspectively understand why certain music affects us in a certain way, or atleast sets up the foiundation for such research.\n",
    "\n",
    "# Data\n",
    "------------------------------\n",
    "\n",
    "The data I'm trying to explore is hosted by the [Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/IFOBRN), titled: **Music and emotion dataset (Primary Musical Cues)**. \n",
    "\n",
    "## I. Summary\n",
    "\n",
    "This Dataset contains the mean emotional response of a listener for a sound wave that is altered iteratively on 7 points. The dataset focuses on initial emotional response. The data is stored in two tables, A **Design Matrix** table that describes the stimuli, and the **Mean emotional response** table that records the initial response for each stimuli other the mean emotional response for each of these stimuli.\n",
    "\n",
    "## II. Table Description\n",
    "\n",
    "### Design Matrix Table\n",
    "\n",
    "The design matric table contains information on each stimulus wave. The dataset contains a total of 200 stimuli waves ([link to stimuli](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/IFOBRN/J2D0BN&version=1.0)), these 200 stimuli waves were created by altering an initial sound wave iteratively and factorially by the following 7 properties \n",
    "\n",
    "- **Register** - 6 levels\n",
    "    - 1: 53 MIDI pitch\n",
    "    - 2: 59 MIDI pitch\n",
    "    - 3: 65 MIDI pitch\n",
    "    - 4: 71 MIDI pitch\n",
    "    - 5: 77 MIDI pitch\n",
    "    - 6: 83 MIDI pitch\n",
    "- **Mode** - 2 modes\n",
    "    - 1: Major key\n",
    "    - 2: Minor key\n",
    "- **Tempo** - 5 tempos (in NPS)\n",
    "    - 1: 1.2\n",
    "    - 2: 2\n",
    "    - 3: 2.8\n",
    "    - 4: 4.4\n",
    "    - 5: 6\n",
    "- **Sound level** - 5 sound levels (in dB)\n",
    "    - 1: -10\n",
    "    - 2: -5\n",
    "    - 3: 0\n",
    "    - 4: +5\n",
    "    - 5: +10\n",
    "- **Articulation** - 4 levels (from legato to staccato)\n",
    "- **Timbre** - 3 levels \n",
    "    - 1: trumpet\n",
    "    - 2: flute\n",
    "    - 3: horn\n",
    "- **Melody** - 4 types\n",
    "    - 1: Sad\n",
    "    - 2: Scary\n",
    "    - 3: Happy\n",
    "    - 4: Peaceful\n",
    "\n",
    "(It must be noted that the subset of 200 stimuli as opposed to a complete factorial was done by the researchers to focus their research on just the first-interaction level of the above variables. It would have been nice to have a complete set of the factorials so I could ask different or more in-depth questions, but this dataset provides enough information for the scope of this research)\n",
    "\n",
    "The Melody is categorized as \"Sad, Happy, Scary, or Peaceful\" based on research conducted previosuly ([Vieillard et al., 2008.](https://www.tandfonline.com/doi/full/10.1080/02699930701503567) The musical excerpts can be used for research with ackowledgements of the copyright, © Bernard Bouchard, 1998.). This study conducted research into the specificties of emotions provoked by the 4 sound waves on their own.\n",
    "\n",
    "The seven alterable factors were decided based on researcg by [Bresin, R. & Friberg, A. (2011). Emotion rendering in music: range and characteristic values of seven musical variables. Cortex, 47(9), 1068-1081](https://www.ncbi.nlm.nih.gov/pubmed/21696717)\n",
    "\n",
    "The table contains 8 columns and 200 tuples, each row co-inciding with one stimuli wave.\n",
    "\n",
    "The columns are : Nro (Stimuli Number), and the 7 characteristics with their respective level:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Nro | Register | Mode | Tempo | Soundlevel | Articulation | Timbre | Melody |\n",
    "|-----|----------|------|-------|------------|--------------|--------|--------|\n",
    "| 1   | 4        | 1    | 4     | 4          | 2            | 2      | 4      |\n",
    "| 2   | 5        | 1    | 4     | 1          | 1            | 2      | 2      |\n",
    "| 3   | 2        | 2    | 5     | 1          | 1            | 2      | 1      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Emotional Response Table\n",
    "\n",
    "This table contains the mean responses categorized into 4 major buckets \"Scary, Happy, Sad, and Peaceful\" for each of the 200 stimuli.\n",
    "\n",
    "As per the [paper published](https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00487/full) with research on this data, 46 listeners were asked to record their emotional response to each sound stimuli, for each emotional category on a scale of 1 to 7. Thus each listener recorded 4 paralell ratings for each sound stimuli.\n",
    "\n",
    "These 46 listeners were spread out across two research labs, one in stockholm the other in Jyväskylä.\n",
    "\n",
    "Thus table contains 5 columns and 200 tuples, with each row corresponding to one sound stimuli.\n",
    "\n",
    "The columns are: Nro (Stimuli number), and mean score out of 7 for each bucket: scary, Happy, Sad and Peaceful.\n",
    "\n",
    "| Nro | Scary  | Happy  | Sad    | Peaceful |\n",
    "|-----|--------|--------|--------|----------|\n",
    "| 1   | 1.2889 | 4.4667 | 1.7111 | 3.1333   |\n",
    "| 2   | 1.0667 | 5.4444 | 1.4889 | 4.4889   |\n",
    "| 3   | 2.0222 | 1.4889 | 3.7778 | 2.7111   |\n",
    "\n",
    "\n",
    "## III. License\n",
    "\n",
    "The data is available under [CC0 - \"Public Domain Dedication\"](https://creativecommons.org/publicdomain/zero/1.0/)\n",
    "The research used to create this data is under a [CC BY 3.0](https://creativecommons.org/licenses/by/3.0/) license, Thus the research can be used as long asthe authors are cited, and due copyright mentioned: © 2013 Eerola, Friberg and Bresin \n",
    "\n",
    "## IV. Possible Biases\n",
    "\n",
    "As per previously conducted studies, such as [The Structure of Musical Preference](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3138530/), barring genre and lyrics there are certain factors such as loudness, beat, tempo that can make up someone's musical preference. In such a situation, this study may just be documenting a preference and how people react to music outside of their preference or that fall under the preference. Emotional response illicited by preferred music can be different, perhaps an additional category stating if the user would ever listen to this stimuli on repeat would explore that confounding/mediating factor.\n",
    "\n",
    "Demographics such as age, location, usual music preference, the kind of music a listener grew up listening to can also affect musical preference and emotional response to music, a more spreadout research group documenting more background specific information could have found a more generic understanding of how music illicits response.\n",
    "\n",
    "Certain tempos and Sound levels were left out of the study to reduce the number of musical samples, although this decision is based on prior research perhaps certain aspects or anomalies of musical stimuli response was missed. \n",
    "\n",
    "# Research Questions\n",
    "------------------------------\n",
    "\n",
    "The Questions I'm interested in asking and answering are:\n",
    "\n",
    "#### -  How linear is the relationship between \"Melody\" and \"Mean Emotional Response\"?\n",
    "\n",
    "    Since the melody is already classified in the same buckets as Mean Emotional Response, it would be interesting to know if the overreaching melody type has a predominant affect on the mean emotional response\n",
    "\n",
    "#### - Which factor most affects the relationship between \"Melody\" and \"Mean Emotional Response\"?\n",
    "\n",
    "    Which of the six musical factors most changes the mean emotional response from co-inciding with the Melody\n",
    "\n",
    "#### - Does a \"minor\" mode mostly illicit a \"Scary\" emotional response?\n",
    "\n",
    "    As a listener of music generally songs in a minor key can come off as creepy, I would like to know if that holds true for the populous\n",
    "\n",
    "#### - Does a high tempo leady to a happier emotional response?\n",
    "\n",
    "    Generally speaking fast tempo-ed songs (such as pop music) geenrate a positive response, it would be interesting to know if that is true here as well\n",
    "\n",
    "#### - Which emotional response is most common, and what factor attributes to that response?\n",
    "\n",
    "    Is there a common emotional response? and if so which factor is most likely to cause it would be an interesting find.\n",
    "\n",
    "These questions although they are what I will try to focus on maybe expanded or changed depending on what I find most interesting on exploring the data\n",
    "\n",
    "\n",
    "# Tools\n",
    "------------------------------\n",
    "\n",
    "I plan on using `python` language, and documenting my work on a `.ipynb` notebook for ease of use and understandability. `matplotlib` will be used for plotting graphs.\n",
    "\n",
    "I hope to use some basic statistcal approaches to find linearity and confounding variables (such as intercept analysis for LR), and basic grouping by and summation for any other analyses.\n",
    "\n",
    "\n",
    "# Sources\n",
    "------------------------------\n",
    "\n",
    "- Emotional expression in music: contribution, linearity, and additivity of primary musical cues: https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00487/full#h8\n",
    "\n",
    "- The Structure of Musical Preferences: A Five-Factor Model : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3138530/\n",
    "\n",
    "- Music and Emotion Dataset: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/IFOBRN\n",
    "\n",
    "- Vieillard, S., Peretz, I., Gosselin, N., Khalfa, S., Gagnon, L., and Bouchard, B. (2008). Happy, sad, scary and peaceful musical excerpts for research on emotions. Cognition and Emotion, 22, 720–752.: https://www.tandfonline.com/doi/full/10.1080/02699930701503567\n",
    "\n",
    "- Bresin, R. & Friberg, A. (2011). Emotion rendering in music: range and characteristic values of seven musical variables. Cortex, 47(9), 1068-1081: https://www.ncbi.nlm.nih.gov/pubmed/21696717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
